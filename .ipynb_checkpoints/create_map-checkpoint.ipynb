{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b7c8753b-2337-4773-9f67-a8a23cafceab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sentence_transformers\n",
    "import torch\n",
    "import collections\n",
    "import igraph\n",
    "import random\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d528542d-afbe-466d-b0c6-5e2d2dcf88b4",
   "metadata": {},
   "source": [
    "Create text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18a3dd81-17d6-4a5f-ac6c-97f377bea389",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"example_data.json\"\n",
    "with open(file_path, \"r\") as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dae8c8ea-c5a7-4a04-8a2f-f95f163e37b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_l = []\n",
    "for i, row in enumerate(data):\n",
    "    d = row['metadata']\n",
    "    if d.get('language') and 'eng' in d['language']:\n",
    "        if d['subject']:\n",
    "            text = d['title'] + '[SEP]' + d['description'] + ' Keywords: ' + ', '.join(d['subject']) + '.'\n",
    "        else:\n",
    "            text = d['title'] + '[SEP]' + d['description']\n",
    "        text_info = (i, d['identifier'], d['title'])\n",
    "        text_l.append((text, text_info))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fef72b-acb1-40ec-ac7e-ecc7cfcdc5b3",
   "metadata": {},
   "source": [
    "Create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2efad2dc-b9be-4eb8-bcb1-f3ab8a0499d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sentence_transformers.SentenceTransformer('allenai-specter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d920b9dd-3c92-4abf-bbda-7de8bd059b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.encode([x[0] for x in text_l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3345c3c0-9476-46f6-8458-cb1bc2df0afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = torch.tensor(embeddings)\n",
    "embeddings = embeddings.to(\"cpu\")\n",
    "embeddings = sentence_transformers.util.normalize_embeddings(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3aa024fb-bc6f-4aad-9702-02b378418694",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_self_matches(doc_matches):\n",
    "    \"\"\"\n",
    "    Adjust sublists within a dictionary of sublists keyed by 'corpus_id'. Each sublist is expected\n",
    "    to initially contain 21 elements. After filtering the self-match, if there are more than 20 elements, \n",
    "    the last element is removed to ensure there are exactly 20.\n",
    "    \n",
    "    :param doc_matches: A list of lists, where each sublist contains dictionaries with keys 'corpus_id' and 'score'.\n",
    "    :return: A dictionary with adjusted sublists, keyed by 'corpus_id'.\n",
    "    \"\"\"\n",
    "    new_doc_matches = {}\n",
    "    for index, sublist in enumerate(doc_matches):\n",
    "        # Filter out the dictionary where corpus_id matches the index (self match)\n",
    "        filtered_sublist = [entry for entry in sublist if entry['corpus_id'] != index]\n",
    "\n",
    "        # If the filtered sublist is longer than 20, remove the last element\n",
    "        if len(filtered_sublist) > 20:\n",
    "            filtered_sublist = filtered_sublist[:20]\n",
    "\n",
    "        # Use the index (which is the corpus_id for the query document) as the key in the dictionary\n",
    "        new_doc_matches[index] = filtered_sublist\n",
    "    return new_doc_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4bfdc7a7-2df5-4442-8f4f-0a8e29e98551",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 21\n",
    "hits = sentence_transformers.util.semantic_search(embeddings, embeddings, top_k=top_k)\n",
    "hits = remove_self_matches(hits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bab0857-5a45-404b-ba2c-e3c818f41914",
   "metadata": {},
   "source": [
    "Create network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "420246cd-823a-4de0-927b-235672c24483",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_d = collections.Counter()\n",
    "for index_node_1, hit_d_l in hits.items():\n",
    "    for hit_d in hit_d_l:\n",
    "        edge_weight = hit_d['score']\n",
    "        index_node_2 = hit_d['corpus_id']\n",
    "        iedge_name = tuple(sorted([index_node_1, index_node_2]))\n",
    "        sum_d[iedge_name] += edge_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "52049c37-dfc7-4154-b438-9763f8b8d45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_Network(nodeweight_l, iedgeid_l, iedgeweight_l):\n",
    "    #nodeid must be the same as node index in nodeweight_l\n",
    "    ig_network = igraph.Graph(directed=False)\n",
    "    ig_network.add_vertices(len(nodeweight_l))\n",
    "    ig_network.vs['weight'] = nodeweight_l\n",
    "    ig_network.vs['name'] = [v.index for v in ig_network.vs]\n",
    "    ig_network.add_edges(iedgeid_l)\n",
    "    ig_network.es['weight'] = iedgeweight_l\n",
    "    return ig_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2f185b1a-d48e-4d10-ae70-116362fe8edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "iedge_l, edgeweight_l = zip(*sum_d.items())\n",
    "ig_network = create_Network([1 for x in hits.keys()], iedge_l, edgeweight_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3fb9d4-0c8b-42cb-b93f-316313953659",
   "metadata": {},
   "source": [
    "Create clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6225589c-a458-478b-bd2c-d12598fb825d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Partition_Class(ig_network, resolution, random_seed=0, node_weights=None, weights=None, n_iterations=2):\n",
    "    \"\"\"Creates an Igraph representation of the network\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ig_network : igraph.Graph object\n",
    "        Igraph representation of the network with edge weight = 1.\n",
    "        \n",
    "    resolution: float\n",
    "        Resolution to be used in the Leiden algorithm clustering.\n",
    "        \n",
    "    random_seed: int, optional\n",
    "        Random seed of the Leiden algorithm clustering.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    partition : igraph.clustering.VertexClustering object\n",
    "        Partition of the nodes into clusters acording to the Leiden algorithm. The clustering is hard-clustering and it can contain singletons.\n",
    "\n",
    "    Notes\n",
    "    -------\n",
    "    Requires the igraph module.\n",
    "    Requires the random module\n",
    "    The purpose of the function is to fix the random seed of the Leiden algorihm so the results of the clustering\n",
    "    become replicable.\n",
    "    \"\"\"\n",
    "    igraph.set_random_number_generator(random)\n",
    "    random.seed(random_seed)\n",
    "    partition = ig_network.community_leiden(resolution=resolution, node_weights=node_weights, weights=weights, n_iterations=n_iterations)\n",
    "    return partition\n",
    "\n",
    "import copy\n",
    "\n",
    "def join_Clusters(clu_d, con_d, n_desired, resolution): # No more need for the resolution argument, discontinue in the future\n",
    "    \"\"\"Creates dictionary of joined clusters\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    clu_d : dict of tuple\n",
    "        Dictionary where the first level is the name of the cluster and the second level is the list of the name of the nodes in the cluster.\n",
    "    \n",
    "    con_d: dict of dict of int\n",
    "        Dictionary where the first level is the name of a given cluster, the second level is the name of another given cluster and the third level\n",
    "        is the number of edges between the clusters.\n",
    "    \n",
    "    n_desired: int\n",
    "        How many clusters you want to have after the joining process.\n",
    "    \n",
    "    resolution: float\n",
    "        Resolution to be used in the Leiden algorithm clustering. It is used to calculate the score of the connections between clusters.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    merge_dict: dict of dict\n",
    "        Dictionary with the output of the merging.\n",
    "        Keys:\n",
    "        \n",
    "        jclu_d: dict of tuple\n",
    "            Dictionary where the first level is the name of the cluster and the second level is the list of the name of the nodes in the cluster.\n",
    "            The clusters contain the nodes after mergin. The number of cluster is the desired number of clusters (n_desired).\n",
    "        \n",
    "        jrem_d: dict of tuple\n",
    "            Dictionary where the first level is the name of the cluster and the second level is the list of the name of the nodes in the cluster.\n",
    "            The clusters contain the nodes after mergin. When a cluster can't be merged, it is placed in this dictionary. The conditions for not merging a\n",
    "            cluster are: It is not in the connections dict or the number of connections to other clusters sum 0.\n",
    "        \n",
    "        jcon_d: dict of dict\n",
    "             Dictionary where the first level is the name of a given cluster, the second level is the name of another given cluster and the third level\n",
    "            is the number of edges between the clusters. It contains the connections between the merged clusters. All the clusters are included, and if\n",
    "            they have no connections, then the connections number is 0.\n",
    "\n",
    "    Notes\n",
    "    -------\n",
    "    Requires de copy module.\n",
    "    The purpose of the function is to merge the clusters until you have a desired number of clusters.\n",
    "    The merging process is:\n",
    "        1- If you have enought clusters, stop.\n",
    "        2- Identify the smallest cluster.\n",
    "        3- Calculate the connectivity score between the smallest cluster and the other clusters.\n",
    "        4- Merge the smallest cluster with the cluster with the best connectivity score.\n",
    "        5- Go to step 1.\n",
    "    There are 2 optimized steps in this function:\n",
    "        A- Identify the smallest cluster: To identify the smallest cluster, I sort a list of the sizes of the clusters. However, the size of the clusters\n",
    "        changes after merging, so I have create a list of the sizes of the clusters and sort it after each merging. The optimization is that most of the \n",
    "        time the smallest cluster of the list will be the same before and after merging. This happens because the clusters usually merge with some of the\n",
    "        bigger clusters, therefore the size of the smaller clusters tend to not change. This is relevant because the vast mayority of the clusters are very\n",
    "        small. I take advantage if this fact in the get_Smallest_Cluster function. It uses te ranking and jclu_s variables to chec if the size of the cluster\n",
    "        to merge is the same since the last time the list was sorted. Because the clusters can't get smaller, the fact that the cluster to merge is the same\n",
    "        size as the last time the list was sorted means that it is time to merge the cluster to merge. If the cluster to merge is not of the same size, it\n",
    "        meanse that there may be another smaller cluster in the list and so the list of clusters sizes has to be created again and sorted to find the smallest\n",
    "        cluster. This list is now the one that will be used in the following evaluations of the cluster to merge.\n",
    "        B- Calculate the connectivity score between the smallest cluster and the other clusters: I already calculated the number of conections between \n",
    "        the clusters once in the con_d variable. Therefore, to not calculate the number of conections again, I add the conections with the other clusters of \n",
    "        the smaller merged to the connections with the other clusters of the bigger merged cluster. I have to be carfull to not add self-connections.\n",
    "    c_m: Name of the cluster to merge\n",
    "    c_b: Name of the cluster with the best score\n",
    "    c_m_c: Name of a given cluster that is also conected to c_m\n",
    "    jclu_s: Dictionary of the size of the clusters. The purpose of this variable is to save the size of the clusters so to not calculate them again each time they are needed.\n",
    "    \"\"\"\n",
    "    jclu_d = copy.deepcopy(clu_d)\n",
    "    jcon_d = copy.deepcopy(con_d)\n",
    "    jrem_d = {}\n",
    "    jclu_s = {c: len(jclu_d[c]) for c in jclu_d}  # clusters size list, used for finding the smallest cluster in an optimized way, see get_Smallest_Cluster function\n",
    "    ref_jclu_s = dict_As_Sorted_Tuples(jclu_s) # Reference size list, used for finding the smallest cluster in an optimized way, see get_Smallest_Cluster function\n",
    "    while len(jclu_d) > n_desired:  # Main loop, stops once you have the number of desired clusters\n",
    "        ref_jclu_s, c_m = get_Smallest_Cluster(ref_jclu_s, jclu_s)  # Which is the smallest cluster? (Optimized)\n",
    "        if c_m in jcon_d:  # If the cluster to merge is in the connections dictionary, procede, else, add the cluster to merge to the rem_d\n",
    "            score_d = {}\n",
    "            for c_m_c in jcon_d[c_m]:\n",
    "                n_con = jcon_d[c_m][c_m_c]\n",
    "                if n_con != 0:  # The importance of this line is that merging with no conections may have better scores than mergins with some conections, and I want to avoid that by omiting clusters with no conections\n",
    "                    score_d[c_m_c] = get_Merging_Resolution(n_con, jclu_s[c_m], jclu_s[c_m_c])\n",
    "            if len(score_d) != 0:  # If the cluster to merge has any connection to the other clusters, procede, else, add the cluster to merge to the rem_d\n",
    "                c_b = max_Key_By_Value(score_d)\n",
    "                jclu_d, jclu_s, jcon_d = upd_Merge(c_m, c_b, jclu_d, jclu_s, jcon_d)  # Merge the clusters\n",
    "            else:\n",
    "                jclu_d, jclu_s, jcon_d, jrem_d = upd_Remove(c_m, jclu_d, jclu_s, jcon_d, jrem_d)  # This line removes the cluster\n",
    "        else:\n",
    "            jclu_d, jclu_s, jcon_d, jrem_d = upd_Remove(c_m, jclu_d, jclu_s, jcon_d, jrem_d)  # This line removes the cluster\n",
    "        del(ref_jclu_s[0])  # Remove the cluster from the ranking list\n",
    "    jcon_d = clean_Con_D(jcon_d, jclu_d)\n",
    "    merge_dict = {'jclu_d': jclu_d, 'jrem_d': jrem_d, 'jcon_d': jcon_d}\n",
    "    return merge_dict\n",
    "\n",
    "def upd_Remove(c_m, jclu_d, jclu_s, jcon_d, jrem_d):\n",
    "    \"\"\"Updates the dictionaries when you have to remove a cluster\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    c_m : int\n",
    "        Name of the cluster to merge\n",
    "    \n",
    "    jclu_d: dict of tuple\n",
    "        Dictionary of clusters merged and clusters to merge where the first level is the name of the cluster and the second level\n",
    "        is the list of the name of the nodes in the cluster.\n",
    "        \n",
    "    jclu_s: dict of int\n",
    "        Dictionary of the size of the clusters\n",
    "\n",
    "    jcon_d: dict of dict\n",
    "        Dictionary where the first level is the name of a given cluster, the second level is the name of another given cluster and the third level\n",
    "        is the number of edges between the clusters.\n",
    "        \n",
    "    jrem_d: dict of tuple\n",
    "        Dictionary of removed clusterswhere the first level is the name of the cluster and the second level is the list of the name \n",
    "        of the nodes in the cluster.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    jclu_d: dict of tuple\n",
    "        Updated jclu_d parameter.\n",
    "        \n",
    "    jclu_s: dict of int\n",
    "        Updated jclu_s parameter.\n",
    "    \n",
    "    jcon_d: dict of dict\n",
    "        Updated jcon_d parameter.\n",
    "    \n",
    "    jrem_d: dict of tuple\n",
    "        Updated jrem_d parameter (c_m cluster name and nodes added).\n",
    "        \n",
    "    Notes\n",
    "    -------\n",
    "    It removes the values of c_m from jclu_s and jclu_d, but before removing the values of c_m from jcon_d it checks if the c_m exists in jcon_d. It also removes the c_m conection from the value of other clustes conected to c_m.\n",
    "    c_m_c: Name of a given cluster that is also conected to c_m\n",
    "    \"\"\"\n",
    "    jrem_d[c_m] = jclu_d[c_m]\n",
    "    del(jclu_s[c_m])\n",
    "    del(jclu_d[c_m])\n",
    "    if c_m in jcon_d:\n",
    "        for c_m_c in jcon_d[c_m]:\n",
    "            del(jcon_d[c_m_c][c_m]) # Remove c_m conection from the value of c_m_c\n",
    "        del(jcon_d[c_m])  # Remove c_m value\n",
    "    return jclu_d, jclu_s, jcon_d, jrem_d\n",
    "\n",
    "def upd_Merge(c_m, c_b, jclu_d, jclu_s, jcon_d):\n",
    "    \"\"\"Updates the dictionaries when you have to merge a cluster\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    c_m : int\n",
    "        Name of the cluster to merge\n",
    "        \n",
    "    c_b : int\n",
    "        Name of the cluster with the best conectivity score\n",
    "    \n",
    "    jclu_d: dict of tuple\n",
    "        Dictionary of clusters merged and clusters to merge where the first level is the name of the cluster and the second level\n",
    "        is the list of the name of the nodes in the cluster.\n",
    "        \n",
    "    jclu_s: dict of int\n",
    "        Dictionary of the size of the clusters\n",
    "\n",
    "    jcon_d: dict of dict\n",
    "        Dictionary where the first level is the name of a given cluster, the second level is the name of another given cluster and the third level\n",
    "        is the number of edges between the clusters.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    jclu_d: dict of tuple\n",
    "        Updated jclu_d parameter.\n",
    "        \n",
    "    jclu_s: dict of int\n",
    "        Updated jclu_s parameter.\n",
    "    \n",
    "    jcon_d: dict of dict\n",
    "        Updated jcon_d parameter.\n",
    "    \"\"\"\n",
    "    jclu_d = merge_Clu_D(c_m, c_b, jclu_d)\n",
    "    jclu_s = merge_Clu_S(c_m, c_b, jclu_s)\n",
    "    jcon_d = merge_Con_D(c_m, c_b, jcon_d)\n",
    "    return jclu_d, jclu_s, jcon_d\n",
    "\n",
    "def merge_Clu_D(c_m, c_b, jclu_d):\n",
    "    \"\"\"Update jclu_d so that c_m and c_b are merged\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    c_m : int\n",
    "        Name of the cluster to merge\n",
    "        \n",
    "    c_b : int\n",
    "        Name of the cluster with the best conectivity score\n",
    "    \n",
    "    jclu_d: dict of tuple\n",
    "        Dictionary of clusters merged and clusters to merge where the first level is the name of the cluster and the second level\n",
    "        is the list of the name of the nodes in the cluster.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    jclu_d: dict of tuple\n",
    "        Updated jclu_d parameter.\n",
    "    \"\"\"\n",
    "    jclu_d[c_b].update(jclu_d[c_m])\n",
    "    del(jclu_d[c_m])\n",
    "    return jclu_d\n",
    "\n",
    "def merge_Clu_S(c_m, c_b, jclu_s):\n",
    "    \"\"\"Update jclu_s so that c_m and c_b are merged\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    c_m : int\n",
    "        Name of the cluster to merge\n",
    "        \n",
    "    c_b : int\n",
    "        Name of the cluster with the best conectivity score\n",
    "    \n",
    "    jclu_s: dict of int\n",
    "        Dictionary of the size of the clusters\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    jclu_s: dict of tuple\n",
    "        Updated jclu_s parameter.\n",
    "    \"\"\"\n",
    "    jclu_s[c_b] += jclu_s[c_m]\n",
    "    del(jclu_s[c_m])\n",
    "    return jclu_s\n",
    "\n",
    "def merge_Con_D(c_m, c_b, jcon_d):\n",
    "    \"\"\"Update jclu_s so that c_m and c_b are merged\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    c_m : int\n",
    "        Name of the cluster to merge\n",
    "        \n",
    "    c_b : int\n",
    "        Name of the cluster with the best conectivity score\n",
    "    \n",
    "    jcon_d: dict of dict\n",
    "        Dictionary where the first level is the name of a given cluster, the second level is the name of another given cluster and the third level\n",
    "        is the number of edges between the clusters.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    jcon_d: dict of tuple\n",
    "        Updated jcon_d parameter.\n",
    "        \n",
    "    Notes\n",
    "    -------\n",
    "    c_m_c:  Name of a given cluster that is also conected to c_m\n",
    "    \"\"\"\n",
    "    for c_m_c in jcon_d[c_m]:  # Remove all conections (c_m_c to c_m), and add that value to the conections (c_b to c_m_c)\n",
    "        if c_m_c != c_b:  # This prevents the cluster for creating a conection with itself.\n",
    "            if c_m_c not in jcon_d[c_b]:  # If the conection does not already exists, create it\n",
    "                jcon_d[c_b][c_m_c] = 0\n",
    "                jcon_d[c_m_c][c_b] = 0\n",
    "            jcon_d[c_b][c_m_c] += jcon_d[c_m][c_m_c]  # Add the value of the conection (c_m to c_m_c) to the conection (c_b to c_m_c)\n",
    "            jcon_d[c_m_c][c_b] += jcon_d[c_m][c_m_c]  # Same as above\n",
    "        del(jcon_d[c_m_c][c_m])  # Remove all conections (c_m_c to c_m)\n",
    "    del(jcon_d[c_m])   # Remove the conections (c_b to any)\n",
    "    return jcon_d\n",
    "\n",
    "def get_Smallest_Cluster(ref_jclu_s, jclu_s):\n",
    "    \"\"\"Get the smallest cluster in an optimized way\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ref_jclu_s : list of tuples\n",
    "        Ordered list of tuples of (cluster name,  cluster size), serves as references for jclus_s. The first tuple is the smallest one.\n",
    "        \n",
    "    jclu_s : dict of int\n",
    "        Dictionary of the size of the clusters\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    out_ref_jclu_s:  list of tuples\n",
    "        Updated ref_jclu_s parameter (or the original one if nothing changed).\n",
    "    \n",
    "    out_c_m: int\n",
    "        Name of the cluster to merge\n",
    "        \n",
    "    Notes\n",
    "    -------\n",
    "    To optimize the proces, the function tries to find the smallest cluster without sorting the sizes in jclu_s.\n",
    "    To do this, it checks if the size of the cluster to merge in jclu_s is the same as it was the last time the list was sorted (i.e. ref_jclu_s)\n",
    "    If it is, then there is no need to sort the list again. Otherwise, it sortes again and upgrades the refference list.\n",
    "    \"\"\"\n",
    "    ref_c_m, ref_c_size = ref_jclu_s[0]  # Get the smallest cluster size and name acording to the reference cluster sizes list\n",
    "    c_size = jclu_s[ref_c_m]  #  Get the size of the cluster from the clusters sizes dict\n",
    "    if c_size == ref_c_size:  #  If the size of the cluster from the actuall cluster list is the same as the size of that cluster in the cluster list, then that is the cluster to merge, if not, you have you sort the list again to find the cluster to merge\n",
    "        out_ref_jclu_s = ref_jclu_s\n",
    "    else:\n",
    "        out_ref_jclu_s = dict_As_Sorted_Tuples(jclu_s)\n",
    "    out_c_m = out_ref_jclu_s[0][0]\n",
    "    return out_ref_jclu_s, out_c_m\n",
    "\n",
    "def get_Merging_Resolution(n_con, c_1_size, c_2_size):\n",
    "    \"\"\"Get the resolution at which the change in the clustering score after merging the clusters is 0\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_con : int\n",
    "        Number of conections between the clusters\n",
    "        \n",
    "    c_1_size : int\n",
    "        Size of one cluster\n",
    "        \n",
    "    c_2_size : int\n",
    "        Size of the other cluster\n",
    "\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    resolution:  float\n",
    "        The resolution at which the chnage in score is 0\n",
    "        \n",
    "    Notes\n",
    "    -------\n",
    "    This is the merging technique that uses the Leiden algorithm. You should merge the pairs of clusters where the resolution is the highest.\n",
    "    The reason is that the highest resolution will be the closest one to the resolution that you are already using. The merging resolution will be\n",
    "    lower than the resolution you are already using because otherwise you would had already merged the clusters.\n",
    "    \"\"\"\n",
    "    pos_n_con = c_1_size*c_2_size # All the positble pairs of nodes in the new cluster\n",
    "    resolution = n_con/pos_n_con\n",
    "    return resolution\n",
    "\n",
    "def get_Score(n_con, c_1_size, c_2_size, resolution): # Discontinued use in the current pipeline\n",
    "    \"\"\"Get the change in the clustering score after merging the clusters\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_con : int\n",
    "        Number of conections between the clusters\n",
    "        \n",
    "    c_1_size : int\n",
    "        Size of one cluster\n",
    "        \n",
    "    c_2_size : int\n",
    "        Size of the other cluster\n",
    "        \n",
    "    resolution: float\n",
    "        Resolution value of the Leiden algorithm\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    score:  float\n",
    "        Change in the clustering score after merging the clusters\n",
    "        \n",
    "    Notes\n",
    "    -------\n",
    "    Returns the score, as used by the Leiden Algorihtm (https://onlinelibrary.wiley.com/doi/full/10.1002/asi.22748, equation 4).\n",
    "    The score is calculated only for the new pairs of nodes that apear after merging the clusters. The non conected pairs add (-resolution) to the score,\n",
    "    while connected pairs add (1-resolution) to the score.\n",
    "    \"\"\"\n",
    "    pos_n_con = c_1_size*c_2_size # All the positble pairs of nodes in the new cluster\n",
    "    score = n_con*(1-resolution) - (pos_n_con-n_con)*resolution  # This works the followin way: For each pair of nodes in the cluster, if they are conected then add (1-resolution), else add (0-resolution)\n",
    "    return score\n",
    "\n",
    "def clean_Con_D(jcon_d, jclu_d):\n",
    "    \"\"\"Makes a connection dictionary that only contains the joined clusters and and have conection values for all of them (i.e. a clean version of jcon_d)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    jcon_d: dict of dict\n",
    "        Dictionary where the first level is the name of a given cluster, the second level is the name of another given cluster and the third level\n",
    "        is the number of edges between the clusters.\n",
    "    \n",
    "    jclu_d: dict of tuple\n",
    "        Dictionary of clusters merged and clusters to merge where the first level is the name of the cluster and the second level\n",
    "        is the list of the name of the nodes in the cluster.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cjcon_d: dict of dict\n",
    "        Clean jcon_d.\n",
    "        \n",
    "    Notes\n",
    "    -------\n",
    "    This step is necesary so jcon_d can be used in follow up clusterings.\n",
    "    \"\"\"\n",
    "    cjcon_d = {}\n",
    "    for c_1 in list(jclu_d):  # Use the keys of jclu_d to make the keys in cjcon_d\n",
    "        cjcon_d[c_1] = {}\n",
    "        for c_2 in list(jclu_d):\n",
    "            if c_1 != c_2:  # Make sure you are not anotation links from the cluster to itself\n",
    "                if c_1 not in jcon_d:  # If the cluster is not in jcon_d, then anotate it in cjcon_d with conection value 0 to the other clusters\n",
    "                    cjcon_d[c_1][c_2] = 0\n",
    "                else:\n",
    "                    if c_2 not in jcon_d[c_1]: # Same as above\n",
    "                        cjcon_d[c_1][c_2] = 0\n",
    "                    else:\n",
    "                        cjcon_d[c_1][c_2] = jcon_d[c_1][c_2]\n",
    "    return cjcon_d\n",
    "\n",
    "def max_Key_By_Value(dictionary):\n",
    "    \"\"\"Find the maximum key in a dict acorting to its value\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dictionary: dict\n",
    "        A given dictionary\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    max_key: any\n",
    "        The maximum key acorting to its value.\n",
    "        \n",
    "    Notes\n",
    "    -------\n",
    "    max_value: The maximum value of the dictionary\n",
    "    \"\"\"\n",
    "    max_value = max([value for value in dictionary.values()])\n",
    "    max_key = max([key for key in dictionary if dictionary[key] == max_value])\n",
    "    return max_key\n",
    "\n",
    "def dict_As_Sorted_Tuples(jclu_s):\n",
    "    \"\"\"Turns jclu_s into a sorted list of tuples\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    jclu_s : dict of int\n",
    "        Dictionary of the size of the clusters\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    sorted_tuples: list of tuples\n",
    "         Ordered list of tuples of (cluster name,  cluster size).\n",
    "\n",
    "    \"\"\"\n",
    "    list_of_tuples = list(jclu_s.items())\n",
    "    sorted_tuples = sorted(list_of_tuples, key=lambda x: x[1])\n",
    "    return sorted_tuples\n",
    "\n",
    "def c_Cluster_D(partition):\n",
    "    \"\"\"Creates clusters dictionary\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    partition : igraph.clustering.VertexClustering object.\n",
    "        Partition of the nodes into clusters.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cluster_d : dict of tuple\n",
    "        Dictionary where the first level is the name of the cluster and the second level is the list of the name of the nodes in the cluster.\n",
    "\n",
    "    Notes\n",
    "    -------\n",
    "    The main purpose of this function is to get the name of the nodes.\n",
    "    cluster_i = Is the cluster index obtained from enumerating the clusters list. I dont know another way of obtaining the cluster index. It becomes the de-facto cluster name.\n",
    "    cluster_nodes_i = Cluster nodes indices list.\n",
    "    node_i = Node index.\n",
    "    \"\"\"\n",
    "    cluster_d = {}\n",
    "    for cluster_i, cluster_nodes_i in enumerate(list(partition)):\n",
    "        cluster_d[cluster_i] = set()\n",
    "        for node_i in cluster_nodes_i:\n",
    "            node_name = partition.graph.vs[node_i]['name']  # This is the critical part of the function\n",
    "            cluster_d[cluster_i].add(node_name)\n",
    "    return cluster_d\n",
    "\n",
    "def c_Connections_D(partition):\n",
    "    \"\"\"Creates connections dictionary\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    partition : igraph.clustering.VertexClustering object\n",
    "        Partition of the nodes into clusters.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    con_d: dict of dict of int\n",
    "        Dictionary where the first level is the index of a given cluster, the second level is the index of another given cluster and the third level\n",
    "        is the number of edges between the clusters. It does not includes pairs of clusters with no conections. The dictionary anotates the number\n",
    "        of edges for both orders of clusters (e.g. both conn_d[1][2] = 3 and conn_d[2][1] = 3). It does not anotates edges from a cluster to itself.\n",
    "\n",
    "    Notes\n",
    "    -------\n",
    "    The function uses igraph.clustering.VertexClustering.cluster_graph() to efficienly obtain the number of edges between the clusters.\n",
    "    The parameter cluster_graph(combine_edges=sum) sums the attributes of the edges. The atrribute of the edges is 'weight', and\n",
    "    the value is '1'. Therefore, the attrribute 'weight' will tell you hom many edges there were originaly.\n",
    "    The index of the clusters is the same as in the c_Cluster_D function output. It becomes the de-facto cluster name.\n",
    "    c_1_i = Cluster 1 index\n",
    "    c_2_i = Cluster 2 index\n",
    "    \"\"\"\n",
    "    con_d = {}\n",
    "    cluster_g = partition.cluster_graph(combine_edges=sum)  # This is the critical part of the function.\n",
    "    for edge in cluster_g.es:\n",
    "        c_1_i = edge.source\n",
    "        c_2_i = edge.target\n",
    "        n_conn = edge['weight']\n",
    "        if c_1_i != c_2_i:  # Don't anotate conections from the cluter to itself.\n",
    "            if c_1_i not in con_d:\n",
    "                con_d[c_1_i] = {}\n",
    "            if c_2_i not in con_d:\n",
    "                con_d[c_2_i] = {}\n",
    "            con_d[c_1_i][c_2_i] = n_conn\n",
    "            con_d[c_2_i][c_1_i] = n_conn\n",
    "    return con_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7c4875f0-3068-4d83-8a6f-454e86b61159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25, 17, 14, 10, 10]\n",
      "0.9156626506024096\n"
     ]
    }
   ],
   "source": [
    "nclusters = 5\n",
    "resolution = 0.5\n",
    "partition = get_Partition_Class(ig_network, resolution, n_iterations=3)\n",
    "print(sorted(partition.sizes(), reverse=True)[:nclusters])\n",
    "print(sum(sorted(partition.sizes(), reverse=True)[:nclusters])/len(ig_network.vs()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7271639f-2a04-4dd8-aefe-f8f1e139a8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "clu_d = c_Cluster_D(partition)\n",
    "con_d = c_Connections_D(partition)\n",
    "merging_data = join_Clusters(clu_d, con_d, nclusters, resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f1bb41-6582-411b-a814-bd63bc8578a0",
   "metadata": {},
   "source": [
    "Label clustes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ae864f4a-7bac-433c-a3d3-fda25fed9a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_noun_phrases(text, nlp=nlp):\n",
    "    \"\"\"\n",
    "    Extract all possible noun phrases from a given text using spaCy.\n",
    "    A noun phrase is defined as any sequence of nouns and adjectives that ends in a noun,\n",
    "    including all sub-phrases.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to process\n",
    "        \n",
    "    Returns:\n",
    "        list: List of all possible noun phrases, including sub-phrases\n",
    "        \n",
    "    Example:\n",
    "        >>> text = \"The big brown dog\"\n",
    "        >>> extract_noun_phrases(text)\n",
    "        ['big brown dog', 'brown dog', 'dog']\n",
    "    \"\"\"\n",
    "    \n",
    "    # Process the text\n",
    "    doc = nlp(text.lower())\n",
    "    \n",
    "    noun_phrases = []\n",
    "    \n",
    "    for i, token in enumerate(doc):\n",
    "        # If we find a noun, look backwards for adjectives and nouns\n",
    "        if token.pos_ in ['NOUN', 'PROPN']:\n",
    "            # Generate all possible phrases ending with this noun\n",
    "            phrase_words = []\n",
    "            j = i\n",
    "            while j >= 0 and doc[j].pos_ in ['ADJ', 'NOUN', 'PROPN']:\n",
    "                phrase_words.insert(0, doc[j].text)\n",
    "                if len(phrase_words) > 1:  # Only add phrases with 2+ words\n",
    "                    noun_phrases.append(' '.join(phrase_words))\n",
    "                j -= 1\n",
    "            # Add the single noun itself\n",
    "            noun_phrases.append(token.text)\n",
    "    \n",
    "    # Remove duplicates while preserving order\n",
    "    seen = set()\n",
    "    noun_phrases = set(x for x in noun_phrases if not (x in seen or seen.add(x)))\n",
    "    \n",
    "    return noun_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "71c4019c-696c-44b4-be41-f210f658fc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_d = {}\n",
    "for node, text in enumerate([x[0] for x in text_l]):\n",
    "    np_d[node] = extract_noun_phrases(text.replace(\"[SEP]\", \". \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2da65793-0617-4030-a210-81ad482e7961",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_np = collections.Counter([np for np_s in np_d.values() for np in np_s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7d603a04-8bff-446f-aff1-d7bc6ab097ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_np_d = {}\n",
    "for cluster, node_s in merging_data['jclu_d'].items():\n",
    "    cluster_np_d[cluster] = collections.Counter([np for np_s in [np_d[node] for node in merging_data['jclu_d'][cluster]] for np in np_s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "64d699d0-c52a-422e-b6cc-72032e297e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 2\n",
    "len_all = len(text_l)\n",
    "cluster_np_score_d = {}\n",
    "for cluster, np_count in cluster_np_d.items():\n",
    "    clu_len = len(merging_data['jclu_d'][cluster])\n",
    "    cluster_np_score_d[cluster] = {}\n",
    "    for np, count in np_count.items():\n",
    "        local_frecuency = count/clu_len\n",
    "        global_frecuency = all_np[np]/len_all\n",
    "        ss_global_frecuency = min(all_np[np]+m, len_all)/len_all\n",
    "        cluster_np_score_d[cluster][np] = (local_frecuency, local_frecuency/global_frecuency, local_frecuency/ss_global_frecuency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "64dfea75-9721-4bd5-a6ae-cdc010dddddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = 'id\\tlabel\\tdescription\\tx\\ty\\tcluster\\tweight<Size>\\n'\n",
    "for i, local_cluster_np_d in cluster_np_score_d.items():\n",
    "    index = i\n",
    "    label = sorted(list(local_cluster_np_d.items()), key=lambda x: -x[1][2])[0][0]\n",
    "    description = '<table><tr><td>Over-rep.</td><td>Cover</td><td>Label</td></tr>'\n",
    "    for row in sorted(list(local_cluster_np_d.items()), key=lambda x: -x[1][2])[:5]:\n",
    "         description += f\"<tr><td>{round(row[1][1])}</td><td>{format(row[1][0], '.2f')}</td><td>{row[0]}</td></tr>\"\n",
    "    description += '</table>'\n",
    "    x = i\n",
    "    y = i\n",
    "    cluster = 1\n",
    "    size = len(clu_d[i])\n",
    "    headers += f\"{index}\\t{label}\\t{description}\\t{x}\\t{y}\\t{cluster}\\t{size}\\n\"\n",
    "with open('map.txt', 'w') as f:\n",
    "    f.write(headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8926dee5-3b01-4106-9952-881a81fcea49",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ''\n",
    "for x, y_d in merging_data['jcon_d'].items():\n",
    "    for y, count in y_d.items():\n",
    "        net += f\"{x}\\t{y}\\t{count}\\n\"\n",
    "with open('net.txt', 'w') as f:\n",
    "    f.write(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ef663c9f-b591-416b-99fa-118d612b5e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_book_d = {}\n",
    "for cluster, local_cluster_np_d in cluster_np_score_d.items():\n",
    "    label = sorted(list(local_cluster_np_d.items()), key=lambda x: -x[1][2])[0][0]\n",
    "    cluster_book_d[label] = [{'title': text_l[node][1][2], 'id': text_l[node][1][1]} for node in merging_data['jclu_d'][cluster]]\n",
    "with open(\"cluster_book_d.json\", \"w\") as json_file:\n",
    "    json.dump(cluster_book_d, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9817a10f-dd73-4ced-9cae-1a5b9b1d751b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
